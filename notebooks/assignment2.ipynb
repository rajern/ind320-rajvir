{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6fa505a",
   "metadata": {},
   "source": [
    "# Project work, part 2 - Data Sources\n",
    "\n",
    "# General\n",
    "\n",
    "- All project work in **IND320** will result in personal hand-ins and online apps.\n",
    "\n",
    "1. **A Jupyter Notebook** run locally on your computer (later with access to online and local databases).\n",
    "   - This will be your basic development and documentation platform.\n",
    "   - Must include a brief description of **AI usage**.\n",
    "   - Must include a **300–500 word log** describing the compulsory work (including both Jupyter Notebook and Streamlit experience).\n",
    "   - Must include **links** to your public **GitHub repository** and **Streamlit app** (see below) for the compulsory work.\n",
    "   - Document headings should be clear and usable for navigation during development.\n",
    "   - All code blocks must include enough **comments** to be understandable and reproducible if someone inherits your project.\n",
    "   - All code blocks must be **run before export to PDF** so messages and plots are shown. In addition, add the `.ipynb` file to the GitHub repository where you have your Streamlit project.\n",
    "\n",
    "2. **A Streamlit app** running from `https://[yourproject].streamlit.app/`.\n",
    "   - This will be an online version of the project, accessing data that has been exported to CSV format (later, also an online database).\n",
    "   - The code, hosted at GitHub, must include relevant comments from the Jupyter Notebook and further comments regarding Streamlit usage.\n",
    "\n",
    "- There are **four parts** in the project work, building on each other and resulting in a final portfolio and app to be presented at the end of the semester.\n",
    "- **Co-operation is applauded**, and the use of **AI tools** is encouraged.\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "- The uploaded **PDF**, **GitHub repository**, and **Streamlit app** will be assessed according to the recipe above.\n",
    "  - By one fellow student in **peer review**.\n",
    "  - By **Liland** or **Kjæreng**.\n",
    "- TA/Teacher’s feedback will be **short and instructive**, regarding points of improvement and fulfilment of requirements.\n",
    "- **Final fulfilment** of the course will be based on the **four rounds of hand-ins** seen as a whole.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c6fe0",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "**Here are the links to Streamlit and Github:**\n",
    "- Streamlit URL: https://ind320-rajvir-a4ycr6bfeb2lv6hsrnmfou.streamlit.app/\n",
    "- Github URL: https://github.com/rajern/ind320-rajvir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfafe6",
   "metadata": {},
   "source": [
    "#### Local database: Cassandra\n",
    "\n",
    "- If not already done, set up Cassandra and Spark as described in the book.\n",
    "- Test that your Spark–Cassandra connection works.\n",
    "- The Cassandra database will be accessed from the Jupyter Notebook and used to store data from the API mentioned later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1158ae4",
   "metadata": {},
   "source": [
    "(Comment: Make sure to activate .venv and start docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14bb4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Add environment variables\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Users\\rajvi\\AppData\\Local\\Programs\\Microsoft\\jdk-17.0.16.8-hotspot\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "\n",
    "# Ensure Spark uses the Python interpreter from the kernel\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# Add Java and winutils to PATH\n",
    "os.environ[\"PATH\"] = rf\"{os.environ['HADOOP_HOME']}\\bin;{os.environ['JAVA_HOME']}\\bin;\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38ea5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Cassandra driver\n",
    "!pip install -q cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4740b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1b12f2338d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to local Cassandra container\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "# Set up new keyspace\n",
    "session.execute(\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS my_first_keyspace\n",
    "WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "# Create new table\n",
    "session.set_keyspace('my_first_keyspace')\n",
    "session.execute(\"DROP TABLE IF EXISTS my_first_table;\")\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS my_first_table (ind int PRIMARY KEY, company text, model text);\")\n",
    "\n",
    "# Insert data into the table\n",
    "session.execute(\"INSERT INTO my_first_table (ind, company, model) VALUES (1, 'Tesla', 'Model 3');\")\n",
    "session.execute(\"INSERT INTO my_first_table (ind, company, model) VALUES (2, 'Tesla', 'Model Y');\")\n",
    "session.execute(\"INSERT INTO my_first_table (ind, company, model) VALUES (3, 'Polestar', '2');\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1aa55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark with connector to Cassandra\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SparkCassandraApp\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "    .config(\"spark.sql.catalog.mycatalog\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e96eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  3|Polestar|      2|\n",
      "|  1|   Tesla|Model 3|\n",
      "|  2|   Tesla|Model Y|\n",
      "+---+--------+-------+\n",
      "\n",
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  2|   Tesla|Model Y|\n",
      "|  3|Polestar|      2|\n",
      "|  1|   Tesla|Model 3|\n",
      "+---+--------+-------+\n",
      "\n",
      "+---+-------+-------+\n",
      "|ind|company|  model|\n",
      "+---+-------+-------+\n",
      "|  2|  Tesla|Model Y|\n",
      "|  1|  Tesla|Model 3|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Cassandra table\n",
    "df = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    "      .options(table=\"my_first_table\", keyspace=\"my_first_keyspace\")\n",
    "      .load())\n",
    "df.show()\n",
    "\n",
    "# Create SQL view and run simple queries\n",
    "df.createOrReplaceTempView(\"my_first_table_view\")\n",
    "spark.sql(\"SELECT * FROM my_first_table_view\").show()\n",
    "spark.sql(\"SELECT * FROM my_first_table_view WHERE company='Tesla'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6bc62",
   "metadata": {},
   "source": [
    "#### Remote database: MongoDB\n",
    "\n",
    "- If not already done, prepare a MongoDB account at [mongodb.com](https://www.mongodb.com).\n",
    "- Test that you can manipulate data from Python.\n",
    "- The MongoDB database will store data that has been trimmed/curated/prepared through the Jupyter Notebook and Spark filtering.\n",
    "- These data will be accessed directly from the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0e3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MongoDG URI\n",
    "from pathlib import Path\n",
    "import tomllib\n",
    "\n",
    "secrets_path = Path(\"../.streamlit/secrets.toml\")\n",
    "uri = tomllib.load(open(secrets_path, \"rb\"))[\"MONGODB_URI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07461346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# Test MongoDB connection\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import streamlit as st\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a157f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('68fa627c523c7983334b0039'), 'test': 'ok'}, {'_id': ObjectId('68fb16576749bf3d2b973bd0'), 'test': 'ok'}, {'_id': ObjectId('68fb1acf6749bf3d2b973bd3'), 'test': 'ok'}, {'_id': ObjectId('68fb28bb14356b602c5cd8c6'), 'test': 'ok'}, {'_id': ObjectId('68fb553014356b602c5cd8c9'), 'test': 'ok'}, {'_id': ObjectId('68fb630e14356b602c5cd8cc'), 'test': 'ok'}, {'_id': ObjectId('68fba6d41f02f734ba223017'), 'test': 'ok'}, {'_id': ObjectId('68fbb67b1f02f734ba22301a'), 'test': 'ok'}, {'_id': ObjectId('68fbb7301f02f734ba22301d'), 'test': 'ok'}, {'_id': ObjectId('68fbc8a91f02f734ba255d81'), 'test': 'ok'}, {'_id': ObjectId('690ce160c420a5f096a0a441'), 'test': 'ok'}]\n"
     ]
    }
   ],
   "source": [
    "# Check that we can manipulate data from python\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "db = client[\"test_db\"]\n",
    "col = db[\"test_collection\"]\n",
    "\n",
    "# Insert one test document\n",
    "col.insert_one({\"test\": \"ok\"})\n",
    "\n",
    "# Read it back\n",
    "print(list(col.find({})))\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb576a",
   "metadata": {},
   "source": [
    "#### API\n",
    "\n",
    "- Familiarise yourself with the API connection at [https://api.elhub.no](https://api.elhub.no).\n",
    "  - Observe how time is encoded and how transitions between summer and winter time are handled.\n",
    "  - Be aware of the time period limitations for each API request and how this differs between datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04dba3",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook\n",
    "\n",
    "- Use the Elhub API to retrieve hourly production data for all price areas using `PRODUCTION_PER_GROUP_MBA_HOUR` for all days and hours of the year 2021.\n",
    "  - Extract only the list in `productionPerGroupMbaHour`, convert to a DataFrame, and insert the data into Cassandra using Spark.\n",
    "  - Use Spark to extract the columns `priceArea`, `productionGroup`, `startTime`, and `quantityKwh` from Cassandra.\n",
    "- Create the following plots:\n",
    "  - A pie chart for the total production of the year from a chosen price area, where each piece of the pie is one of the production groups.\n",
    "  - A line plot for the first month of the year for a chosen price area. Make separate lines for each production group.\n",
    "- Insert the Spark-extracted data into your MongoDB.\n",
    "- Remember to fill in the log and AI mentioned in the General section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0ffed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 2021-01-01 → 2021-01-31 ...\n",
      "Fetching 2021-02-01 → 2021-02-28 ...\n",
      "Fetching 2021-03-01 → 2021-03-31 ...\n",
      "Fetching 2021-04-01 → 2021-04-30 ...\n",
      "Fetching 2021-05-01 → 2021-05-31 ...\n",
      "Fetching 2021-06-01 → 2021-06-30 ...\n",
      "Fetching 2021-07-01 → 2021-07-31 ...\n",
      "Fetching 2021-08-01 → 2021-08-31 ...\n",
      "Fetching 2021-09-01 → 2021-09-30 ...\n",
      "Fetching 2021-10-01 → 2021-10-31 ...\n",
      "Fetching 2021-11-01 → 2021-11-30 ...\n",
      "Fetching 2021-12-01 → 2021-12-31 ...\n",
      "---------------------------------------------------------------------------\n",
      "Rows: 208248\n",
      "                     endTime            lastUpdatedTime priceArea  \\\n",
      "0  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "1  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "2  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "3  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "4  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
      "\n",
      "  productionGroup  quantityKwh                  startTime  \n",
      "0           hydro    2507716.8  2021-01-01T00:00:00+01:00  \n",
      "1           hydro    2494728.0  2021-01-01T01:00:00+01:00  \n",
      "2           hydro    2486777.5  2021-01-01T02:00:00+01:00  \n",
      "3           hydro    2461176.0  2021-01-01T03:00:00+01:00  \n",
      "4           hydro    2466969.2  2021-01-01T04:00:00+01:00  \n"
     ]
    }
   ],
   "source": [
    "# Fetch data from Elhub API \n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# API endpoint\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "DATASET = \"PRODUCTION_PER_GROUP_MBA_HOUR\"  # API dataset name\n",
    "\n",
    "def month_ranges(year: int):\n",
    "    \"\"\"Yield (start_date, end_date) for each month of given year as YYYY-MM-DD strings.\"\"\"\n",
    "    months = pd.date_range(f\"{year}-01-01\", f\"{year}-12-31\", freq=\"MS\")\n",
    "    for start in months:\n",
    "        end = (start + pd.offsets.MonthEnd(1)).normalize()\n",
    "        yield start.date().isoformat(), end.date().isoformat()\n",
    "\n",
    "all_records = []\n",
    "for start_date, end_date in month_ranges(2021):\n",
    "    params = {\n",
    "        \"dataset\": DATASET,\n",
    "        \"startDate\": start_date,   # YYYY-MM-DD\n",
    "        \"endDate\": end_date        # YYYY-MM-DD\n",
    "    }\n",
    "    print(f\"Fetching {start_date} → {end_date} ...\")\n",
    "    resp = requests.get(BASE_URL, params=params, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    payload = resp.json()\n",
    "\n",
    "    # Extract the inner list with hourly rows\n",
    "    for item in payload.get(\"data\", []):\n",
    "        rows = item[\"attributes\"].get(\"productionPerGroupMbaHour\", [])\n",
    "        all_records.extend(rows)\n",
    "\n",
    "    # To keep within API rate limits\n",
    "    time.sleep(0.2)\n",
    "\n",
    "# Convert API response to DataFrame\n",
    "df_raw = pd.DataFrame(all_records)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"-\"*75)\n",
    "print(f\"Rows: {len(df_raw)}\")\n",
    "print(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf8fdb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priceArea                  object\n",
      "productionGroup            object\n",
      "startTime          datetime64[ns]\n",
      "quantityKwh               float64\n",
      "dtype: object\n",
      "  priceArea productionGroup           startTime  quantityKwh\n",
      "0       NO1           hydro 2020-12-31 23:00:00    2507716.8\n",
      "1       NO1           hydro 2021-01-01 00:00:00    2494728.0\n",
      "2       NO1           hydro 2021-01-01 01:00:00    2486777.5\n",
      "3       NO1           hydro 2021-01-01 02:00:00    2461176.0\n",
      "4       NO1           hydro 2021-01-01 03:00:00    2466969.2\n",
      "       priceArea productionGroup           startTime  quantityKwh\n",
      "208243       NO5            wind 2021-12-30 18:00:00          0.0\n",
      "208244       NO5            wind 2021-12-30 19:00:00          0.0\n",
      "208245       NO5            wind 2021-12-30 20:00:00          0.0\n",
      "208246       NO5            wind 2021-12-30 21:00:00          0.0\n",
      "208247       NO5            wind 2021-12-30 22:00:00          0.0\n"
     ]
    }
   ],
   "source": [
    "# Keep only the required columns\n",
    "df_clean = df_raw[[\"priceArea\", \"productionGroup\", \"startTime\", \"quantityKwh\"]].copy()\n",
    "\n",
    "# Parse startTime as UTC and drop tz info (Cassandra expects naive UTC)\n",
    "df_clean[\"startTime\"] = pd.to_datetime(df_clean[\"startTime\"], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Check the data\n",
    "print(df_clean.dtypes)\n",
    "print(df_clean.head())\n",
    "print(df_clean.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e62aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra table ready\n"
     ]
    }
   ],
   "source": [
    "# Set up Cassandra keyspace and table for Elhub data\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "KEYSPACE = \"elhub2021\"\n",
    "TABLE = \"prod_by_group_hour\"\n",
    "\n",
    "# Connect to local Cassandra\n",
    "session = Cluster([\"localhost\"], port=9042).connect()\n",
    "\n",
    "# Create keyspace and table if they don't exist\n",
    "session.execute(\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS elhub2021\n",
    "WITH REPLICATION = {'class':'SimpleStrategy','replication_factor':1};\n",
    "\"\"\")\n",
    "session.set_keyspace(KEYSPACE)\n",
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS prod_by_group_hour (\n",
    "  pricearea text,\n",
    "  productiongroup text,\n",
    "  starttime timestamp,\n",
    "  quantitykwh double,\n",
    "  PRIMARY KEY ((pricearea, productiongroup), starttime)\n",
    ") WITH CLUSTERING ORDER BY (starttime ASC);\n",
    "\"\"\")\n",
    "print(\"Cassandra table ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "412d399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to Cassandra.\n"
     ]
    }
   ],
   "source": [
    "# Write data to Cassandra using Spark\n",
    "df_spark = spark.createDataFrame(df_clean).toDF(\"pricearea\",\"productiongroup\",\"starttime\",\"quantitykwh\")\n",
    "(df_spark.write\n",
    "  .format(\"org.apache.spark.sql.cassandra\")\n",
    "  .options(keyspace=KEYSPACE, table=TABLE)\n",
    "  .mode(\"append\")\n",
    "  .save())\n",
    "\n",
    "print(\"Data written to Cassandra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49518b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pricearea: string (nullable = false)\n",
      " |-- starttime: timestamp (nullable = true)\n",
      " |-- productiongroup: string (nullable = true)\n",
      " |-- quantitykwh: double (nullable = true)\n",
      "\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|pricearea|starttime          |productiongroup|quantitykwh|\n",
      "+---------+-------------------+---------------+-----------+\n",
      "|NO5      |2020-12-31 23:00:00|hydro          |4068096.5  |\n",
      "|NO5      |2020-12-31 23:00:00|other          |0.0        |\n",
      "|NO5      |2020-12-31 23:00:00|solar          |3.72       |\n",
      "|NO5      |2020-12-31 23:00:00|thermal        |77742.0    |\n",
      "|NO5      |2021-01-01 00:00:00|hydro          |4104306.0  |\n",
      "+---------+-------------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from Cassandra\n",
    "KEYSPACE = \"elhub2021\"\n",
    "TABLE = \"prod_by_group_hour\"\n",
    "\n",
    "df_read = (\n",
    "    spark.read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=KEYSPACE, table=TABLE)\n",
    "    .load()\n",
    ")\n",
    "# Print the schema and show some rows\n",
    "df_read.printSchema()\n",
    "df_read.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431aef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajvi\\AppData\\Local\\Temp\\ipykernel_30640\\1124199018.py:18: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Pie chart for area NO3\n",
    "AREA = \"NO3\"\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate kWh per production group for the chosen area\n",
    "by_group = (df_read\n",
    "            .filter(F.col(\"pricearea\") == AREA)\n",
    "            .groupBy(\"productiongroup\")\n",
    "            .agg(F.sum(\"quantitykwh\").alias(\"kwh\")))\n",
    "\n",
    "pdf = by_group.toPandas().sort_values(\"kwh\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.pie(pdf[\"kwh\"], labels=pdf[\"productiongroup\"], autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(f\"Production share – {AREA} (2021)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc47f01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajvi\\AppData\\Local\\Temp\\ipykernel_30640\\577171127.py:28: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Line plot for January for area NO3, one line per production group\n",
    "AREA = \"NO3\"\n",
    "YEAR = 2021\n",
    "MONTH = 1\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate per hour and group\n",
    "jan = (df_read\n",
    "       .filter((F.col(\"pricearea\")==AREA) &\n",
    "               (F.year(\"starttime\")==YEAR) &\n",
    "               (F.month(\"starttime\")==MONTH))\n",
    "       .groupBy(\"starttime\",\"productiongroup\")\n",
    "       .agg(F.sum(\"quantitykwh\").alias(\"kwh\")))\n",
    "\n",
    "pdf = jan.toPandas()\n",
    "if pdf.empty:\n",
    "    print(f\"No data for {AREA} {YEAR}-{MONTH:02d}\")\n",
    "else:\n",
    "    pivot = pdf.pivot(index=\"starttime\", columns=\"productiongroup\", values=\"kwh\").sort_index()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    pivot.plot(ax=plt.gca())\n",
    "    plt.title(f\"Hourly production – {AREA} {YEAR}-{MONTH:02d}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"kWh\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b82f5564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 208224 rows from Cassandra into MongoDB.\n"
     ]
    }
   ],
   "source": [
    "# Insert Spark-extracted data into MongoDB\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "df_pd = df_read.toPandas()\n",
    "records = df_pd.to_dict(orient=\"records\")\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "client.admin.command(\"ping\")\n",
    "\n",
    "db = client[\"elhub2021\"]\n",
    "col = db[\"production_per_group_hour\"]\n",
    "col.delete_many({})        \n",
    "col.insert_many(records)   \n",
    "print(f\"Inserted {len(records)} rows from Cassandra into MongoDB.\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e7b5a",
   "metadata": {},
   "source": [
    "#### Streamlit app\n",
    "\n",
    "- Update your Streamlit app from part 1 of the project according to the following points.\n",
    "- If you have something you would like to keep on page four, move it to a new page five.\n",
    "- Establish a connection with your MongoDB database. When running this at [streamlit.io](https://streamlit.io), remember to copy your secrets to the webpage instead of exposing them on GitHub.\n",
    "- On page four, split the view into two columns using `st.columns`.\n",
    "  - On the left side, use radio buttons (`st.radio`) to select a price area and display a pie chart like in the Jupyter Notebook.\n",
    "  - On the right side, use pills (`st.pills`) to select which production groups to include and a selection element of your choice to select a month.\n",
    "  - Combine the price area, production group(s) and month, and display a line plot like in the Jupyter Notebook (but for any month).\n",
    "  - Below the columns, insert an expander (`st.expander`) where you briefly document the source of the data shown on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75161f2",
   "metadata": {},
   "source": [
    "### Work Log:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba811e23",
   "metadata": {},
   "source": [
    "**Docker, Cassandra, and Spark** \n",
    "- To begin, I had to set up Docker. After installing Docker Desktop and enabling WSL integration, I pulled and started a Cassandra container locally. This allowed me to run Cassandra on my machine through Docker, using port 9042 for communication.\n",
    "\n",
    "- Next, I set up Apache Spark. I installed Java (JDK 17) since Spark requires it, and then created a virtual environment where I installed all the necessary Python packages, including pyspark and cassandra-driver.\n",
    "\n",
    "- Because this was my first time working with both Cassandra and Spark, I had to spend time understanding how they work individually and how they can be connected. I also learned how Docker functions as a local host and how Spark can communicate with Cassandra through the DataStax Spark Cassandra Connector.\n",
    "\n",
    "- Using the lecture notes and some back-and-forth testing (including help from AI), I gradually got everything to work together. After some debugging and configuration changes, I successfully established a connection between Spark and Cassandra. I was able to read and write data between them, confirming that the integration worked correctly.\n",
    "\n",
    "**MongoDB**\n",
    "- For MongoDB, I followed the instructions from the lecture notes. I created a new user in MongoDB Atlas and then tested the connection using the example code provided.\n",
    "\n",
    "- Initially, I encountered an error because I had written the connection URI incorrectly — I had included the password inside < > brackets instead of writing it directly. After removing those brackets and using the correct format, the connection worked.\n",
    "\n",
    "**Jupyter Notebook**\n",
    "- I used Python and Jupyter Notebook to fetch energy production data from the Elhub API for 2021. I cleaned the data to keep only the most relevant columns and stored it in Cassandra using Spark. Then I read the data back from Cassandra and inserted it into MongoDB. Finally, I created simple visualizations — a pie chart showing production share for one area and a line chart for January, displaying hourly production for each production group.\n",
    "- For this part I had to collaborate with another student, which helped, but this was still my original work. \n",
    "\n",
    "**Streamlit app**\n",
    "- In this part, I created a simple Streamlit web app to visualize the data stored in MongoDB.\n",
    "I reused most of the structure from Assignment 1 but added a new page called Production explorer.\n",
    "This page connects to my MongoDB database using the URI stored in secrets.toml, and shows two plots:\n",
    "a pie chart of total production by group for 2021 and a line chart of hourly production per month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff8f8f",
   "metadata": {},
   "source": [
    "### AI Usage (brief):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cad05",
   "metadata": {},
   "source": [
    "**Docker, Cassandra, and Spark** \n",
    "- I used AI a lot back and forth just to understand how Docker, Cassandra and Spark worked together. Also better understanding the lecture notes and the code written there, which I used directly when creating the tests. I encountered a lot of errors at this stage so I used AI to do debugging, which was helpful. \n",
    "\n",
    "**MongoDB**\n",
    "- Since I just followed the instructions it mostly went well. When I encountered the password error AI managed to help me resolve that issue. \n",
    "\n",
    "**Jupyter Notebook**\n",
    "- Used AI to write code more efficiently + debugging when encountering issues. \n",
    "\n",
    "**Streamlit app**\n",
    "- Used AI to help me write the code in 4_Production Explorer.py. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
